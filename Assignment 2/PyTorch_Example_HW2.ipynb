{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmMYUzfTNcKG"
      },
      "source": [
        "# GloVe Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj6_lyUun7k4"
      },
      "source": [
        "## Download GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHATZ4a6ONE0",
        "outputId": "35890175-41b1-4ffe-894b-8a2d6241b455"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScTsMlAEOSwl",
        "outputId": "c3047e09-f05e-4fcb-a4e9-bfa6ae4c5d86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py-j3cP6oDKI"
      },
      "source": [
        "## Transform GloVe embeddings to Word2Vec format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbrKc9rlNo_D",
        "outputId": "2e563cbe-d6b8-4f41-97db-f63e9e86c87f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mtsourma\\AppData\\Local\\Temp\\ipykernel_17244\\1543968405.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, w2v_output_file) #convert GloVe vectors into the word2vec\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(400000, 50)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import pandas as pd\n",
        "\n",
        "glove_input_file = 'glove.6B.50d.txt'\n",
        "w2v_output_file = 'glv_with_w2v_format.txt'\n",
        "\n",
        "glove2word2vec(glove_input_file, w2v_output_file) #convert GloVe vectors into the word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np58nMYNoMM9"
      },
      "source": [
        "Observe word and respective vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "pCzb77MmRsY9",
        "outputId": "1443ddf0-09bb-4f73-e96e-0bb37324ed94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>vector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>[0.013441, 0.23682, -0.16899, 0.40951, 0.63812...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>[0.15164, 0.30177, -0.16763, 0.17684, 0.31719,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>[0.70853, 0.57088, -0.4716, 0.18048, 0.54449, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>[0.68047, -0.039263, 0.30186, -0.17792, 0.4296...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>and</td>\n",
              "      <td>[0.26818, 0.14346, -0.27877, 0.016257, 0.11384...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>in</td>\n",
              "      <td>[0.33042, 0.24995, -0.60874, 0.10923, 0.036372...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a</td>\n",
              "      <td>[0.21705, 0.46515, -0.46757, 0.10082, 1.0135, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"</td>\n",
              "      <td>[0.25769, 0.45629, -0.76974, -0.37679, 0.59272...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'s</td>\n",
              "      <td>[0.23727, 0.40478, -0.20547, 0.58805, 0.65533,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>[0.15272, 0.36181, -0.22168, 0.066051, 0.13029...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-</td>\n",
              "      <td>[-0.16768, 1.2151, 0.49515, 0.26836, -0.4585, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>that</td>\n",
              "      <td>[0.88387, -0.14199, 0.13566, 0.098682, 0.51218...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>on</td>\n",
              "      <td>[0.30045, 0.25006, -0.16692, 0.1923, 0.026921,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>is</td>\n",
              "      <td>[0.6185, 0.64254, -0.46552, 0.3757, 0.74838, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>was</td>\n",
              "      <td>[0.086888, -0.19416, -0.24267, -0.33391, 0.567...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>said</td>\n",
              "      <td>[0.38973, -0.2121, 0.51837, 0.80136, 1.0336, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>with</td>\n",
              "      <td>[0.25616, 0.43694, -0.11889, 0.20345, 0.41959,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>he</td>\n",
              "      <td>[-0.20092, -0.060271, -0.61766, -0.8444, 0.578...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>as</td>\n",
              "      <td>[0.20782, 0.12713, -0.30188, -0.23125, 0.30175...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>it</td>\n",
              "      <td>[0.61183, -0.22072, -0.10898, -0.052967, 0.508...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word                                             vector\n",
              "0    the  [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0...\n",
              "1      ,  [0.013441, 0.23682, -0.16899, 0.40951, 0.63812...\n",
              "2      .  [0.15164, 0.30177, -0.16763, 0.17684, 0.31719,...\n",
              "3     of  [0.70853, 0.57088, -0.4716, 0.18048, 0.54449, ...\n",
              "4     to  [0.68047, -0.039263, 0.30186, -0.17792, 0.4296...\n",
              "5    and  [0.26818, 0.14346, -0.27877, 0.016257, 0.11384...\n",
              "6     in  [0.33042, 0.24995, -0.60874, 0.10923, 0.036372...\n",
              "7      a  [0.21705, 0.46515, -0.46757, 0.10082, 1.0135, ...\n",
              "8      \"  [0.25769, 0.45629, -0.76974, -0.37679, 0.59272...\n",
              "9     's  [0.23727, 0.40478, -0.20547, 0.58805, 0.65533,...\n",
              "10   for  [0.15272, 0.36181, -0.22168, 0.066051, 0.13029...\n",
              "11     -  [-0.16768, 1.2151, 0.49515, 0.26836, -0.4585, ...\n",
              "12  that  [0.88387, -0.14199, 0.13566, 0.098682, 0.51218...\n",
              "13    on  [0.30045, 0.25006, -0.16692, 0.1923, 0.026921,...\n",
              "14    is  [0.6185, 0.64254, -0.46552, 0.3757, 0.74838, 0...\n",
              "15   was  [0.086888, -0.19416, -0.24267, -0.33391, 0.567...\n",
              "16  said  [0.38973, -0.2121, 0.51837, 0.80136, 1.0336, -...\n",
              "17  with  [0.25616, 0.43694, -0.11889, 0.20345, 0.41959,...\n",
              "18    he  [-0.20092, -0.060271, -0.61766, -0.8444, 0.578...\n",
              "19    as  [0.20782, 0.12713, -0.30188, -0.23125, 0.30175...\n",
              "20    it  [0.61183, -0.22072, -0.10898, -0.052967, 0.508..."
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(w2v_output_file, 'r') as infile:\n",
        "    w2v = infile.read().splitlines()\n",
        "\n",
        "w2v_dict = {\n",
        "    'word': [],\n",
        "    'vector': []\n",
        "}\n",
        "\n",
        "for word_vector_pair in w2v[1:22]:\n",
        "    word_vector_pair = word_vector_pair.split()\n",
        "    word = word_vector_pair[0]\n",
        "    vector = word_vector_pair[1:]\n",
        "    w2v_dict['word'].append(word)\n",
        "    w2v_dict['vector'].append(vector)\n",
        "\n",
        "w2v_df = pd.DataFrame.from_dict(w2v_dict)\n",
        "\n",
        "w2v_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0.418',\n",
              " '0.24968',\n",
              " '-0.41242',\n",
              " '0.1217',\n",
              " '0.34527',\n",
              " '-0.044457',\n",
              " '-0.49688',\n",
              " '-0.17862',\n",
              " '-0.00066023',\n",
              " '-0.6566',\n",
              " '0.27843',\n",
              " '-0.14767',\n",
              " '-0.55677',\n",
              " '0.14658',\n",
              " '-0.0095095',\n",
              " '0.011658',\n",
              " '0.10204',\n",
              " '-0.12792',\n",
              " '-0.8443',\n",
              " '-0.12181',\n",
              " '-0.016801',\n",
              " '-0.33279',\n",
              " '-0.1552',\n",
              " '-0.23131',\n",
              " '-0.19181',\n",
              " '-1.8823',\n",
              " '-0.76746',\n",
              " '0.099051',\n",
              " '-0.42125',\n",
              " '-0.19526',\n",
              " '4.0071',\n",
              " '-0.18594',\n",
              " '-0.52287',\n",
              " '-0.31681',\n",
              " '0.00059213',\n",
              " '0.0074449',\n",
              " '0.17778',\n",
              " '-0.15897',\n",
              " '0.012041',\n",
              " '-0.054223',\n",
              " '-0.29871',\n",
              " '-0.15749',\n",
              " '-0.34758',\n",
              " '-0.045637',\n",
              " '-0.44251',\n",
              " '0.18785',\n",
              " '0.0027849',\n",
              " '-0.18411',\n",
              " '-0.11514',\n",
              " '-0.78581']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_df[\"vector\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC76FU3eoVvH"
      },
      "source": [
        "## Examples of semantically similar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Tf53sI0XYvBQ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDPBts_GZNna",
        "outputId": "230935d9-f12d-4e62-9b54-8eb58547b116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most semantically similar to word Woman:\n",
            "\n",
            "     word            similarity score\n",
            "===========================================\n",
            "     girl           0.9065280556678772\n",
            "     man            0.8860336542129517\n",
            "     mother         0.8763704299926758\n",
            "     her            0.8613135814666748\n",
            "     boy            0.859611988067627\n",
            "     she            0.8430695533752441\n",
            "     herself        0.8224568367004395\n",
            "     child          0.8108214139938354\n",
            "     wife           0.8037394285202026\n",
            "     old            0.7982394695281982\n",
            "\n",
            "\n",
            "Most semantically similar to word Man:\n",
            "\n",
            "     word            similarity score\n",
            "===========================================\n",
            "     woman          0.8860337734222412\n",
            "     boy            0.8564431071281433\n",
            "     another        0.8452839851379395\n",
            "     old            0.8372183442115784\n",
            "     one            0.827606201171875\n",
            "     who            0.8244695663452148\n",
            "     him            0.8194693922996521\n",
            "     turned         0.8154467940330505\n",
            "     whose          0.811974048614502\n",
            "     himself        0.807725727558136\n"
          ]
        }
      ],
      "source": [
        "most_similar_woman = model.most_similar('woman')\n",
        "most_similar_man = model.most_similar('man')\n",
        "\n",
        "print(f'Most semantically similar to word Woman:')\n",
        "print(f'\\n{\" \"*5}word{\" \"*10}  similarity score')\n",
        "print(f'{\"=\"*43}')\n",
        "for w in most_similar_woman:\n",
        "    print(f'{\" \"*5}{w[0]:15}{w[1]}')\n",
        "\n",
        "print(f'\\n\\nMost semantically similar to word Man:')\n",
        "print(f'\\n{\" \"*5}word{\" \"*10}  similarity score')\n",
        "print(f'{\"=\"*43}')\n",
        "for w in most_similar_man:\n",
        "    print(f'{\" \"*5}{w[0]:15}{w[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCCFzwnaW5Z"
      },
      "source": [
        "Calculate the following *semantic* equation:\n",
        " $(king - man) + woman = ?$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPBTVggSaQ50",
        "outputId": "aec11074-f172-4baf-dc0b-3f7a4e7be88f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886)]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Find the top-N most similar words. Positive words contribute positively towards the similarity, negative words negatively.\n",
        "\n",
        "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=2) \n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VNmnrbYLhK2"
      },
      "source": [
        "## Visualise Linear Substructures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "obMG5dyf1i0H",
        "outputId": "2ad51717-6121-4496-f656-cf348f2c9e24"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 792x648 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set_context(\"talk\",  font_scale=0.6)\n",
        "\n",
        "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
        "\n",
        "def glove_plot(model):\n",
        "    labels = []\n",
        "    wordvecs = []\n",
        "\n",
        "    for word in vocab:\n",
        "        wordvecs.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42) #T-distributed Stochastic Neighbor Embedding\n",
        "    coordinates = tsne_model.fit_transform(wordvecs) #The fit method is calculating the mean and variance of each of the features present in our data. The transform method is transforming all the features using the respective mean and variance.\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in coordinates:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    \n",
        "    plt.figure(figsize=(11,9)) \n",
        "    plt.title('Linear substructures of words with GloVe embeddings')\n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(2, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n",
        "\n",
        "glove_plot(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruWJ4cTz794Z"
      },
      "source": [
        "# Training a Neural Network with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "E4yVVSMZ7YGi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn #Base class for all neural network modules.\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D43h2egN8qm9"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "8pm7sprm8Tvb",
        "outputId": "d1e53501-9305-4ea8-f9f5-4b9024c0f0ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-119.562108</td>\n",
              "      <td>35.625225</td>\n",
              "      <td>28.589353</td>\n",
              "      <td>2643.664412</td>\n",
              "      <td>539.410824</td>\n",
              "      <td>1429.573941</td>\n",
              "      <td>501.221941</td>\n",
              "      <td>3.883578</td>\n",
              "      <td>207300.912353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.005166</td>\n",
              "      <td>2.137340</td>\n",
              "      <td>12.586937</td>\n",
              "      <td>2179.947071</td>\n",
              "      <td>421.499452</td>\n",
              "      <td>1147.852959</td>\n",
              "      <td>384.520841</td>\n",
              "      <td>1.908157</td>\n",
              "      <td>115983.764387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-124.350000</td>\n",
              "      <td>32.540000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>14999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-121.790000</td>\n",
              "      <td>33.930000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1462.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>790.000000</td>\n",
              "      <td>282.000000</td>\n",
              "      <td>2.566375</td>\n",
              "      <td>119400.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-118.490000</td>\n",
              "      <td>34.250000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>2127.000000</td>\n",
              "      <td>434.000000</td>\n",
              "      <td>1167.000000</td>\n",
              "      <td>409.000000</td>\n",
              "      <td>3.544600</td>\n",
              "      <td>180400.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-118.000000</td>\n",
              "      <td>37.720000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>3151.250000</td>\n",
              "      <td>648.250000</td>\n",
              "      <td>1721.000000</td>\n",
              "      <td>605.250000</td>\n",
              "      <td>4.767000</td>\n",
              "      <td>265000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>-114.310000</td>\n",
              "      <td>41.950000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>37937.000000</td>\n",
              "      <td>6445.000000</td>\n",
              "      <td>35682.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>15.000100</td>\n",
              "      <td>500001.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          longitude      latitude  housing_median_age   total_rooms  \\\n",
              "count  17000.000000  17000.000000        17000.000000  17000.000000   \n",
              "mean    -119.562108     35.625225           28.589353   2643.664412   \n",
              "std        2.005166      2.137340           12.586937   2179.947071   \n",
              "min     -124.350000     32.540000            1.000000      2.000000   \n",
              "25%     -121.790000     33.930000           18.000000   1462.000000   \n",
              "50%     -118.490000     34.250000           29.000000   2127.000000   \n",
              "75%     -118.000000     37.720000           37.000000   3151.250000   \n",
              "max     -114.310000     41.950000           52.000000  37937.000000   \n",
              "\n",
              "       total_bedrooms    population    households  median_income  \\\n",
              "count    17000.000000  17000.000000  17000.000000   17000.000000   \n",
              "mean       539.410824   1429.573941    501.221941       3.883578   \n",
              "std        421.499452   1147.852959    384.520841       1.908157   \n",
              "min          1.000000      3.000000      1.000000       0.499900   \n",
              "25%        297.000000    790.000000    282.000000       2.566375   \n",
              "50%        434.000000   1167.000000    409.000000       3.544600   \n",
              "75%        648.250000   1721.000000    605.250000       4.767000   \n",
              "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
              "\n",
              "       median_house_value  \n",
              "count        17000.000000  \n",
              "mean        207300.912353  \n",
              "std         115983.764387  \n",
              "min          14999.000000  \n",
              "25%         119400.000000  \n",
              "50%         180400.000000  \n",
              "75%         265000.000000  \n",
              "max         500001.000000  "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('california_housing_train.csv')\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EIP1OaJOFak",
        "outputId": "0857eaa6-abe0-4ede-893b-9c76f3675612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape: torch.Size([17000, 8])\n",
            "y shape: torch.Size([17000, 1])\n"
          ]
        }
      ],
      "source": [
        "#Fill NaN values\n",
        "data = data.fillna(0)\n",
        "#Normalize values\n",
        "data = (data-data.mean())/data.std()\n",
        "#Separate features and targets\n",
        "x_df = pd.DataFrame(data, columns=data.columns[:-1])\n",
        "y_df = pd.DataFrame(data, columns=[data.columns[-1]]) #target = 'median_house_value'\n",
        "#Save in tensors\n",
        "x = torch.tensor(x_df.values, dtype=torch.float)\n",
        "y = torch.tensor(y_df.values, dtype=torch.float)\n",
        "\n",
        "print(f\"x shape: {x.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10vl9KnOwfE"
      },
      "source": [
        "## Create a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wTlyzUiG8p5V"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(D_in, H1) # You can also try include activation functions to check how your model will behave\n",
        "        self.linear2 = nn.Linear(H1, H2)\n",
        "        self.linear3 = nn.Linear(H2, H3)\n",
        "        self.linear4 = nn.Linear(H3, D_out)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h1 = self.linear1(x)\n",
        "        h2 = self.linear2(h1)\n",
        "        h3 = self.linear3(h2)\n",
        "        out = self.linear4(h3)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BXAcc2dLPPUe"
      },
      "outputs": [],
      "source": [
        "#Define layer sizes\n",
        "D_in = x.shape[1]\n",
        "H1 = 128 #size of the input sample\n",
        "H2 = 64\n",
        "H3 = 32\n",
        "D_out = 1\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 1e-4 # You can also experiment with different learning rates\n",
        "\n",
        "#Initialise model, loss, optimizer\n",
        "model = Net(D_in, H1, H2, H3, D_out)\n",
        "loss_func = nn.MSELoss(reduction='sum') # You can also try BCELoss and BCEWithLogitsLoss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # You can also try Adam and AdamW\n",
        "\n",
        "#Initialise dataloader\n",
        "dataset = torch.utils.data.TensorDataset(x, y) #class to represent the data as list of tensors. x=input_features, y=labels\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ENFPjgHSCAV",
        "outputId": "e208b5aa-cef3-4028-d5c5-132a99c61591"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (linear1): Linear(in_features=8, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (linear4): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWzZl_LRTd6J"
      },
      "source": [
        "## Train Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fck--1cFP1Rd",
        "outputId": "9167b0cb-c3e1-4d4a-ad0b-e8e392054786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Loss = 29.09840\n",
            "Epoch   1: Loss = 23.78519\n",
            "Epoch   2: Loss = 23.30187\n",
            "Epoch   3: Loss = 23.23951\n",
            "Epoch   4: Loss = 23.29651\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "  batch_losses = []\n",
        "\n",
        "  for x_batch, y_batch in dataloader:\n",
        "    y_pred = model(x_batch)\n",
        "    \n",
        "    loss = loss_func(y_pred, y_batch)\n",
        "    batch_losses.append(loss.item())\n",
        "    # print('y_pred=', y_pred[0])\n",
        "    #Delete previously stored gradients\n",
        "    optimizer.zero_grad()\n",
        "    #Perform backpropagation starting from the loss calculated in this epoch\n",
        "    loss.backward()\n",
        "    #Update model's weights based on the gradients calculated during backprop\n",
        "    optimizer.step()\n",
        "  \n",
        "  print(f\"Epoch {epoch:3}: Loss = {sum(batch_losses)/len(dataloader):.5f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optuna framework sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optuna concepts\n",
        "- Objective function: This function contains all the code of the machine learning task you want to optimize hyperparameters for, i.e. in our case it includes the training and validation loop for the neural network model.\n",
        "- Trial: A trial is a single call of the objective function with a certain set of hyperparameters.\n",
        "- Study: This represents a set of trials to be run. At the end of the study the trials can be compared and the best one(s) chosen. The best trials are the ones minimizing / maximizing the loss function for a machine learning model.\n",
        "- Parameter: In our experiments we are trying to find the best combination of parameters to optimize our objective function. We do this by creating a study which consists of multiple trials, each of which runs with a different set of (hyper)parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Optuna sample\n",
        "\n",
        "def suggest_hyperparameters(trial): #function to include the suggested hyperparameters\n",
        "    # Experimenting with different optimizers\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"AdamW\"])\n",
        "    return optimizer_name\n",
        "\n",
        "#Optuna sample\n",
        "def objective(trial):    \n",
        "    optimizer_name = suggest_hyperparameters(trial)\n",
        "    #Define layer sizes\n",
        "    D_in = x.shape[1]\n",
        "    H1 = 128 #size of the input sample\n",
        "    H2 = 64\n",
        "    H3 = 32\n",
        "    D_out = 1\n",
        "\n",
        "    #Define Hyperparameters\n",
        "    learning_rate = 1e-4 # You can also experiment with different learning rates\n",
        "\n",
        "    #Initialise model, loss, optimizer\n",
        "    model = Net(D_in, H1, H2, H3, D_out)\n",
        "    loss_func = nn.MSELoss(reduction='sum') # You can also try BCELoss and BCEWithLogitsLoss\n",
        "    if optimizer_name == \"Adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == \"SGD\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # You can also try Adam and AdamW\n",
        "\n",
        "    #Initialise dataloader\n",
        "    dataset = torch.utils.data.TensorDataset(x, y) #class to represent the data as list of tensors. x=input_features, y=labels\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True) \n",
        "    for epoch in range(5):\n",
        "        batch_losses = []\n",
        "\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            y_pred = model(x_batch)\n",
        "            \n",
        "            loss = loss_func(y_pred, y_batch)\n",
        "            batch_losses.append(loss.item())\n",
        "            # print('y_pred=', y_pred[0])\n",
        "            #Delete previously stored gradients\n",
        "            optimizer.zero_grad()\n",
        "            #Perform backpropagation starting from the loss calculated in this epoch\n",
        "            loss.backward()\n",
        "            #Update model's weights based on the gradients calculated during backprop\n",
        "            optimizer.step()\n",
        "        \n",
        "        print(f\"Epoch {epoch:3}: Loss = {sum(batch_losses)/len(dataloader):.5f}\")\n",
        "        epoch_loss = sum(batch_losses)/len(dataloader)\n",
        "    return epoch_loss\n",
        "\n",
        "optuna_sample = optuna.create_study(direction = 'minimize' , study_name = 'lr-minim-sample')\n",
        "optuna_sample.optimize(objective, n_trials = 50) #the first parameter is the function that we want to optimise\n",
        "print('numbers of the finished trials:' , len(optuna_sample.trials))\n",
        "print('the best params:' , optuna_sample.best_trial.params)\n",
        "print('the best value:' , optuna_sample.best_value)\n",
        "\n",
        "# Best number of trials appears to be 50 , as it gave me the best hyperparameters for my model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
